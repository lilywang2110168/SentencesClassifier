The precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater to richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the early 19th century, and luxury hotels began to spring up in the later part of the 19th century.

Facsimile transmission systems for still photographs pioneered methods of mechanical scanning of images in the early 19th century. Alexander Bain introduced the facsimile machine between 1843 and 1846. Frederick Bakewell demonstrated a working laboratory version in 1851.[citation needed] Willoughby Smith discovered the photoconductivity of the element selenium in 1873. As a 23-year-old German university student, Paul Julius Gottlieb Nipkow proposed and patented the Nipkow disk in 1884.This was a spinning disk with a spiral pattern of holes in it, so each hole scanned a line of the image. Although he never built a working model of the system, variations of Nipkow's spinning-disk "image rasterizer" became exceedingly common.Constantin Perskyi had coined the word television in a paper read to the International Electricity Congress at the International World Fair in Paris on 25 August 1900. Perskyi's paper reviewed the existing electromechanical technologies, mentioning the work of Nipkow and others.However, it was not until 1907 that developments in amplification tube technology by Lee de Forest and Arthur Korn, among others, made the design practical.

The first demonstration of the live transmission of images was by Georges Rignoux and A. Fournier in Paris in 1909. A matrix of 64 selenium cells, individually wired to a mechanical commutator, served as an electronic retina. In the receiver, a type of Kerr cell modulated the light and a series of variously angled mirrors attached to the edge of a rotating disc scanned the modulated beam onto the display screen. A separate circuit regulated synchronization. The 8x8 pixel resolution in this proof-of-concept demonstration was just sufficient to clearly transmit individual letters of the alphabet. An updated image was transmitted "several times" each second. In 1921 Edouard Belin sent the first image via radio waves with his belinograph.


A camera is an optical instrument for recording or capturing images, which may be stored locally, transmitted to another location, or both. The images may be individual still photographs or sequences of images constituting videos or movies. The camera is a remote sensing device as it senses subjects without physical contact. The word camera comes from camera obscura, which means "dark chamber" and is the Latin name of the original device for projecting an image of external reality onto a flat surface. The modern photographic camera evolved from the camera obscura. The functioning of the camera is very similar to the functioning of the human eye. The first permanent photograph of a camera image was made in 1826 by Joseph Nicéphore Niépce.

A laptop, often called a notebook or "notebook computer", is a small, portable personal computer with a "clamshell" form factor, an alphanumeric keyboard on the lower part of the "clamshell" and a thin LCD or LED computer screen on the upper portion, which is opened up to use the computer. Laptops are folded shut for transportation, and thus are suitable for mobile use.[1] Although originally there was a distinction between laptops and notebooks, the former being bigger and heavier than the latter, as of 2014, there is often no longer any difference.[2] Laptops are commonly used in a variety of settings, such as at work, in education, in playing games, Internet surfing, for personal multimedia and general home computer use.

A standard laptop combines the components, inputs, outputs, and capabilities of a desktop computer, including the display screen, small speakers, a keyboard, hard disk drive, optical disc drive pointing devices (such as a touchpad or trackpad), a processor, and memory into a single unit. Most 2016-era laptops also have integrated webcams and built-in microphones. Some 2016-era laptops have touchscreens. Laptops can be powered either from an internal battery or by an external power supply from an AC adapter. Hardware specifications, such as the processor speed and memory capacity, significantly vary between different types, makes, models and price points. Design elements, form factor and construction can also vary significantly between models depending on intended use. Examples of specialized models of laptops include rugged notebooks for use in construction or military applications, as well as low production cost laptops such as those from the One Laptop per Child organization, which incorporate features like solar charging and semi-flexible components not found on most laptop computers. Portable computers, which later developed into modern laptops, were originally considered to be a small niche market, mostly for specialized field applications, such as in the military, for accountants, or for traveling sales representatives. As portable computers evolved into the modern laptop, they became widely used for a variety of purposes.


An electric battery is a device consisting of one or more electrochemical cells with external connections provided to power electrical devices such as flashlights, smartphones, and electric cars. When a battery is supplying electric power, its positive terminal is the cathode and its negative terminal is the anode.The terminal marked negative is the source of electrons that when connected to an external circuit will flow and deliver energy to an external device. When a battery is connected to an external circuit, electrolytes are able to move as ions within, allowing the chemical reactions to be completed at the separate terminals and so deliver energy to the external circuit. It is the movement of those ions within the battery which allows current to flow out of the battery to perform work.Historically the term "battery" specifically referred to a device composed of multiple cells, however the usage has evolved additionally to include devices composed of a single cell.

Primary (single-use or "disposable") batteries are used once and discarded; the electrode materials are irreversibly changed during discharge. Common examples are the alkaline battery used for flashlights and a multitude of portable electronic devices. Secondary (rechargeable) batteries can be discharged and recharged multiple times using mains power from a wall socket; the original composition of the electrodes can be restored by reverse current. Examples include the lead-acid batteries used in vehicles and lithium-ion batteries used for portable electronics such as laptops and smartphones.

Batteries come in many shapes and sizes, from miniature cells used to power hearing aids and wristwatches to small, thin cells used in smartphones, to large lead acid batteries used in cars and trucks, and at the largest extreme, huge battery banks the size of rooms that provide standby or emergency power for telephone exchanges and computer data centers.

According to a 2005 estimate, the worldwide battery industry generates US$48 billion in sales each year, with 6% annual growth.

Batteries have much lower specific energy (energy per unit mass) than common fuels such as gasoline. This is somewhat offset by the higher efficiency of electric motors in producing mechanical work, compared to combustion engines.

I started reading a library copy and halfway thru I knew I had to BUY a copy.
I've had this laptop for a few months now
I bought this last minute because my old laptop died all of a sudden! 
I replaced a 20+ year old Sleep Number mattress with this mattress. 
I bought this mattress for a trundle in a twin size, 6" thickness. 
I purchased this for my teenager. 

A knife (plural knives) is a tool with a cutting edge or blade, hand-held or otherwise, with most having a handle. Some types of knives are used as utensils, including knives used at the dining table (e.g., butter knives and steak knives) and knives used in the kitchen (e.g., paring knife, bread knife, cleaver). Many types of knives are used as tools, such as the utility knife carried by soldiers, the pocket knife carried by hikers and the hunting knife used by hunters. Knives are also used as a traditional or religious implement, such as the kirpan. Some types of knives are used as weapons, such as daggers or switchblades. Some types of knives are used as sports equipment (e.g., throwing knives). Knives are also used in agriculture, food harvesting etc.; the sickle, the scythe and even the combine harvester are knives.

Knife-like tools were used at least two-and-a-half million years ago, as evidenced by the Oldowan tools.Originally made of rock, bone, flint, and obsidian, knives have evolved in construction as technology has, with blades being made from bronze, copper, iron, steel, ceramics, and titanium. Many cultures have their unique version of the knife. Due to its role as humankind's first tool, certain cultures have attached spiritual and religious significance to the knife.

Most modern-day knives follow either a fixed-blade or a folding construction style, with blade patterns and styles as varied as their makers and countries of origin. The word knife possibly descends from an old Norse word knifr for blade.



Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879.[15] His parents were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein & Cie, a company that manufactured electrical equipment based on direct current.

The Einsteins were non-observant Ashkenazi Jews, and Albert attended a Catholic elementary school in Munich from the age of 5 for three years. At the age of 8, he was transferred to the Luitpold Gymnasium (now known as the Albert Einstein Gymnasium), where he received advanced primary and secondary school education until he left the German Empire seven years later.

In 1894, Hermann and Jakob's company lost a bid to supply the city of Munich with electrical lighting because they lacked the capital to convert their equipment from the direct current (DC) standard to the more efficient alternating current (AC) standard.The loss forced the sale of the Munich factory. In search of business, the Einstein family moved to Italy, first to Milan and a few months later to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method. He later wrote that the spirit of learning and creative thought was lost in strict rote learning. At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note.During his time in Italy he wrote a short essay with the title "On the Investigation of the State of the Ether in a Magnetic Field".

In 1895, at the age of 16, Einstein took the entrance examinations for the Swiss Federal Polytechnic in Zürich (later the Eidgenössische Technische Hochschule, ETH). He failed to reach the required standard in the general part of the examination, but obtained exceptional grades in physics and mathematics.On the advice of the principal of the Polytechnic, he attended the Argovian cantonal school (gymnasium) in Aarau, Switzerland, in 1895–96 to complete his secondary schooling. While lodging with the family of professor Jost Winteler, he fell in love with Winteler's daughter, Marie. (Albert's sister Maja later married Winteler's son Paul.) In January 1896, with his father's approval, Einstein renounced his citizenship in the German Kingdom of Württemberg to avoid military service.In September 1896, he passed the Swiss Matura with mostly good grades, including a top grade of 6 in physics and mathematical subjects, on a scale of 1–6.Though only 17, he enrolled in the four-year mathematics and physics teaching diploma program at the Zürich Polytechnic. Marie Winteler moved to Olsberg, Switzerland, for a teaching post.

Einstein's future wife, Mileva Marić, also enrolled at the Polytechnic that year. She was the only woman among the six students in the mathematics and physics section of the teaching diploma course. Over the next few years, Einstein and Marić's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest. In 1900, Einstein was awarded the Zürich Polytechnic teaching diploma, but Marić failed the examination with a poor grade in the mathematics component, theory of functions.There have been claims that Marić collaborated with Einstein on his 1905 papers, known as the Annus Mirabilis papers, but historians of physics who have studied the issue find no evidence that she made any substantive contributions.


Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signals, in contrast to the totally analog and channel separated signals used by analog television. Due to data compression digital TV can support more than one program in the same channel bandwidth.[125] It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s.[126] Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It was not until the 1990s that digital TV became feasible.[127]

In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies' technologies. Until June 1990, the Japanese MUSE standard, based on an analog system, was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.

In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images.(7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels.(8)The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.

The final standards adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This compromise resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—would be best suited for the newer digital HDTV compatible display devices.[128] Interlaced scanning, which had been specifically designed for older analogue CRT display technologies, scans even-numbered lines first, then odd-numbered ones. In fact, interlaced scanning can be looked at as the first video compression model as it was partly designed in the 1940s to double the image resolution to exceed the limitations of the television broadcast bandwidth. Another reason for its adoption was to limit the flickering on early CRT screens whose phosphor coated screens could only retain the image from the electron scanning gun for a relatively short duration.[129] However interlaced scanning does not work as efficiently on newer display devices such as Liquid-crystal (LCD), for example, which are better suited to a more frequent progressive refresh rate.[128]

Progressive scanning, the format that the computer industry had long adopted for computer display monitors, scans every line in sequence, from top to bottom. Progressive scanning in effect doubles the amount of data generated for every full screen displayed in comparison to interlaced scanning by painting the screen in one pass in 1/60 second, instead of two passes in 1/30 second. The computer industry argued that progressive scanning is superior because it does not "flicker" on the new standard of display devices in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offered a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format. William F. Schreiber, who was director of the Advanced Television Research Program at the Massachusetts Institute of Technology from 1983 until his retirement in 1990, thought that the continued advocacy of interlaced equipment originated from consumer electronics companies that were trying to get back the substantial investments they made in the interlaced technology.[130]

Digital television transition started in late 2000s. All governments across the world set the deadline for analog shutdown by 2010s. Initially the adoption rate was low, as the first digital tuner-equipped TVs were costly. But soon, as the price of digital-capable TVs dropped, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.

The basic idea of using three monochrome images to produce a color image had been experimented with almost as soon as black-and-white televisions had first been built. Although he gave no practical details, among the earliest published proposals for television was one by Maurice Le Blanc, in 1880, for a color system, including the first mentions in television literature of line and frame scanning.[104] Polish inventor Jan Szczepanik patented a color television system in 1897, using a selenium photoelectric cell at the transmitter and an electromagnet controlling an oscillating mirror and a moving prism at the receiver. But his system contained no means of analyzing the spectrum of colors at the transmitting end, and could not have worked as he described it.[105] Another inventor, Hovannes Adamian, also experimented with color television as early as 1907. The first color television project is claimed by him,[106] and was patented in Germany on 31 March 1908, patent № 197183, then in Britain, on 1 April 1908, patent № 7219,[107] in France (patent № 390326) and in Russia in 1910 (patent № 17912).[108]

Scottish inventor John Logie Baird demonstrated the world's first color transmission on 3 July 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with filters of a different primary color; and three light sources at the receiving end, with a commutator to alternate their illumination.[109] Baird also made the world's first color broadcast on 4 February 1938, sending a mechanically scanned 120-line image from Baird's Crystal Palace studios to a projection screen at London's Dominion Theatre.[110] Mechanically scanned color television was also demonstrated by Bell Laboratories in June 1929 using three complete systems of photoelectric cells, amplifiers, glow-tubes, and color filters, with a series of mirrors to superimpose the red, green, and blue images into one full color image.

The first practical hybrid system was again pioneered by John Logie Baird. In 1940 he publicly demonstrated a color television combining a traditional black-and-white display with a rotating colored disk. This device was very "deep", but was later improved with a mirror folding the light path into an entirely practical device resembling a large conventional console.[111] However, Baird was not happy with the design, and, as early as 1944, had commented to a British government committee that a fully electronic device would be better.

In 1939, Hungarian engineer Peter Carl Goldmark introduced an electro-mechanical system while at CBS, which contained an Iconoscope sensor. The CBS field-sequential color system was partly mechanical, with a disc made of red, blue, and green filters spinning inside the television camera at 1,200 rpm, and a similar disc spinning in synchronization in front of the cathode ray tube inside the receiver set.[112] The system was first demonstrated to the Federal Communications Commission (FCC) on 29 August 1940, and shown to the press on 4 September.[113][114][115][116]

CBS began experimental color field tests using film as early as 28 August 1940, and live cameras by 12 November.[117] NBC (owned by RCA) made its first field test of color television on 20 February 1941. CBS began daily color field tests on 1 June 1941.[118] These color systems were not compatible with existing black-and-white television sets, and, as no color television sets were available to the public at this time, viewing of the color field tests was restricted to RCA and CBS engineers and the invited press. The War Production Board halted the manufacture of television and radio equipment for civilian use from 22 April 1942 to 20 August 1945, limiting any opportunity to introduce color television to the general public.[119][120]

As early as 1940, Baird had started work on a fully electronic system he called Telechrome. Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-color image could be obtained. He also demonstrated the same system using monochrome signals to produce a 3D image (called "stereoscopic" at the time). A demonstration on 16 August 1944 was the first example of a practical color television system. Work on the Telechrome continued and plans were made to introduce a three-gun version for full color. However, Baird's untimely death in 1946 ended development of the Telechrome system.[121][122] Similar concepts were common through the 1940s and 1950s, differing primarily in the way they re-combined the colors generated by the three guns. The Geer tube was similar to Baird's concept, but used small pyramids with the phosphors deposited on their outside faces, instead of Baird's 3D patterning on a flat surface. The Penetron used three layers of phosphor on top of each other and increased the power of the beam to reach the upper layers when drawing those colors. The Chromatron used a set of focusing wires to select the colored phosphors arranged in vertical stripes on the tube.

One of the great technical challenges of introducing color broadcast television was the desire to conserve bandwidth, potentially three times that of the existing black-and-white standards, and not use an excessive amount of radio spectrum. In the United States, after considerable research, the National Television Systems Committee[123] approved an all-electronic Compatible color system developed by RCA, which encoded the color information separately from the brightness information and greatly reduced the resolution of the color information in order to conserve bandwidth. The brightness image remained compatible with existing black-and-white television sets at slightly reduced resolution, while color televisions could decode the extra information in the signal and produce a limited-resolution color display. The higher resolution black-and-white and lower resolution color images combine in the brain to produce a seemingly high-resolution color image. The NTSC standard represented a major technical achievement.


Color bars used in a test pattern, sometimes used when no program material is available.
Although all-electronic color was introduced in the U.S. in 1953,[124] high prices, and the scarcity of color programming, greatly slowed its acceptance in the marketplace. The first national color broadcast (the 1954 Tournament of Roses Parade) occurred on 1 January 1954, but during the following ten years most network broadcasts, and nearly all local programming, continued to be in black-and-white. It was not until the mid-1960s that color sets started selling in large numbers, due in part to the color transition of 1965 in which it was announced that over half of all network prime-time programming would be broadcast in color that fall. The first all-color prime-time season came just one year later. In 1972, the last holdout among daytime network programs converted to color, resulting in the first completely all-color network season.

Early color sets were either floor-standing console models or tabletop versions nearly as bulky and heavy; so in practice they remained firmly anchored in one place. The introduction of GE's relatively compact and lightweight Porta-Color set in the spring of 1966 made watching color television a more flexible and convenient proposition. In 1972, sales of color sets finally surpassed sales of black-and-white sets. Color broadcasting in Europe was not standardized on the PAL format until the 1960s, and broadcasts did not start until 1967. By this point many of the technical problems in the early sets had been worked out, and the spread of color sets in Europe was fairly rapid. By the mid-1970s, the only stations broadcasting in black-and-white were a few high-numbered UHF stations in small markets, and a handful of low-power repeater stations in even smaller markets such as vacation spots. By 1979, even the last of these had converted to color and, by the early 1980s, B&W sets had been pushed into niche markets, notably low-power uses, small portable sets, or for use as video monitor screens in lower-cost consumer equipment. By the late 1980s even these areas switched to color sets.

Digital
Main article: Digital television
See also: Digital television transition
Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signals, in contrast to the totally analog and channel separated signals used by analog television. Due to data compression digital TV can support more than one program in the same channel bandwidth.[125] It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s.[126] Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It was not until the 1990s that digital TV became feasible.[127]

In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies' technologies. Until June 1990, the Japanese MUSE standard, based on an analog system, was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.

In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images.(7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels.(8)The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.

The final standards adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This compromise resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—would be best suited for the newer digital HDTV compatible display devices.[128] Interlaced scanning, which had been specifically designed for older analogue CRT display technologies, scans even-numbered lines first, then odd-numbered ones. In fact, interlaced scanning can be looked at as the first video compression model as it was partly designed in the 1940s to double the image resolution to exceed the limitations of the television broadcast bandwidth. Another reason for its adoption was to limit the flickering on early CRT screens whose phosphor coated screens could only retain the image from the electron scanning gun for a relatively short duration.[129] However interlaced scanning does not work as efficiently on newer display devices such as Liquid-crystal (LCD), for example, which are better suited to a more frequent progressive refresh rate.[128]

Progressive scanning, the format that the computer industry had long adopted for computer display monitors, scans every line in sequence, from top to bottom. Progressive scanning in effect doubles the amount of data generated for every full screen displayed in comparison to interlaced scanning by painting the screen in one pass in 1/60 second, instead of two passes in 1/30 second. The computer industry argued that progressive scanning is superior because it does not "flicker" on the new standard of display devices in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offered a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format. William F. Schreiber, who was director of the Advanced Television Research Program at the Massachusetts Institute of Technology from 1983 until his retirement in 1990, thought that the continued advocacy of interlaced equipment originated from consumer electronics companies that were trying to get back the substantial investments they made in the interlaced technology.[130]

Digital television transition started in late 2000s. All governments across the world set the deadline for analog shutdown by 2010s. Initially the adoption rate was low, as the first digital tuner-equipped TVs were costly. But soon, as the price of digital-capable TVs dropped, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.

t has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers.[70] Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. It has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.[which?]

In Steven Johnson's book, Everything Bad Is Good for You, he argues that video games in fact demand far more from a player than traditional games like Monopoly. To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books.[71] Some research suggests video games may even increase players' attention capacities.[72]

Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system.[73] It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits.[74] Students are found to be "learning by doing" while playing video games while fostering creative thinking.[75]

The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people.[76] According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to do problem solving. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.[77]

The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups.[78] In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects.[78] A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games.[79] Action gamers are not only better at ignoring distractions, but also at focusing on the main task.[8

A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.

The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld computing devices. Specialized video games such as arcade games, in which the video game components are housed in a large, typically coin-operated chassis, while common in the 1980s in video arcades, have gradually declined due to the widespread availability of affordable home video game consoles (e.g., PlayStation 4, Xbox One and Nintendo Wii U) and video games on desktop and laptop computers and smartphones.

The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, and buttons, or even, with the Kinect sensor, a person's hands and body. Players typically view the game on a video screen or television or computer monitor, or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and, in the 2010s, voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets. In the 2010s, the video game industry is of increasing commercial importance, with growth driven particularly by the emerging Asian markets and mobile games, which are played on smartphones. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.

The first permanent photograph of a camera image was made in 1826 by Joseph Nicéphore Niépce using a sliding wooden box camera made by Charles and Vincent Chevalier in Paris.[14] Niépce had been experimenting with ways to fix the images of a camera obscura since 1816. The photograph Niépce succeeded in creating shows the view from his window. It was made using an 8-hour exposure on pewter coated with bitumen.[15] Niépce called his process "heliography".[16] Niépce corresponded with the inventor Louis-Jacques-Mande Daguerre, and the pair entered into a partnership to improve the heliographic process. Niépce had experimented further with other chemicals, to improve contrast in his heliographs. Daguerre contributed an improved camera obscura design, but the partnership ended when Niépce died in 1833.[17] Daguerre succeeded in developing a high-contrast and extremely sharp image by exposing on a plate coated with silver iodide, and exposing this plate again to mercury vapor.[18] By 1837, he was able to fix the images with a common salt solution. He called this process Daguerreotype, and tried unsuccessfully for a couple years to commercialize it. Eventually, with help of the scientist and politician François Arago, the French government acquired Daguerre's process for public release. In exchange, pensions were provided to Daguerre as well as Niépce's son, Isidore.[19]

In the 1830s, the English scientist Henry Fox Talbot independently invented a process to fix camera images using silver salts.[20] Although dismayed that Daguerre had beaten him to the announcement of photography, on January 31, 1839 he submitted a pamphlet to the Royal Institution entitled Some Account of the Art of Photogenic Drawing, which was the first published description of photography. Within two years, Talbot developed a two-step process for creating photographs on paper, which he called calotypes. The calotyping process was the first to utilize negative prints, which reverse all values in the photograph - black shows up as white and vice versa.[21] Negative prints allow, in principle, unlimited duplicates of the positive print to be made.[22] Calotyping also introduced the ability for a printmaker to alter the resulting image through retouching.[23] Calotypes were never as popular or widespread as daguerreotypes,[24] owing mainly to the fact that the latter produced sharper details.[25] However, because daguerreotypes only produce a direct positive print, no duplicates can be made. It is the two-step negative/positive process that formed the basis for modern photography.[26]

The first photographic camera developed for commercial manufacture was a daguerreotype camera, built by Alphonse Giroux in 1839. Giroux signed a contract with Daguerre and Isidore Niépce to produce the cameras in France,[27] with each device and accessories costing 400 francs.[28] The camera was a double-box design, with a landscape lens fitted to the outer box, and a holder for a ground glass focusing screen and image plate on the inner box. By sliding the inner box, objects at various distances could be brought to as sharp a focus as desired. After a satisfactory image had been focused on the screen, the screen was replaced with a sensitized plate. A knurled wheel controlled a copper flap in front of the lens, which functioned as a shutter. The early daguerreotype cameras required long exposure times, which in 1839 could be from 5 to 30 minutes.[27][29]

After the introduction of the Giroux daguerreotype camera, other manufacturers quickly produced improved variations. Charles Chevalier, who had earlier provided Niépce with lenses, created in 1841 a double-box camera using a half-sized plate for imaging. Chevalier’s camera had a hinged bed, allowing for half of the bed to fold onto the back of the nested box. In addition to having increased portability, the camera had a faster lens, bringing exposure times down to 3 minutes, and a prism at the front of the lens, which allowed the image to be laterally correct.[30] Another French design emerged in 1841, created by Marc Antoine Gaudin. The Nouvel Appareil Gaudin camera had a metal disc with three differently-sized holes mounted on the front of the lens. Rotating to a different hole effectively provided variable f-stops, letting in different amount of light into the camera.[31] Instead of using nested boxes to focus, the Gaudin camera used nested brass tubes.[32] In Germany, Peter Friedrich Voigtländer designed an all-metal camera with a conical shape that produced circular pictures of about 3 inches in diameter. The distinguishing characteristic of the Voigtländer camera was its use of a lens designed by Joseph Petzval.[33] The f/3.5 Petzval lens was nearly 30 times faster than any other lens of the period, and was the first to be made specifically for portraiture. Its design was the most widely used for portraits until Carl Zeiss introduced the anastigmat lens in 1889.[34]

Within a decade of being introduced in America, 3 general forms of camera were in popular use: the American- or chamfered-box camera, the Robert’s-type camera or “Boston box”, and the Lewis-type camera. The American-box camera had beveled edges at the front and rear, and an opening in the rear where the formed image could be viewed on ground glass. The top of the camera had hinged doors for placing photographic plates. Inside there was one available slot for distant objects, and another slot in the back for close-ups. The lens was focused either by sliding or with a rack and pinion mechanism. The Robert’s-type cameras were similar to the American-box, except for having a knob-fronted worm gear on the front of the camera, which moved the back box for focusing. Many Robert’s-type cameras allowed focusing directly on the lens mount. The third popular daguerreotype camera in America was the Lewis-type, introduced in 1851, which utilized a bellows for focusing. The main body of the Lewis-type camera was mounted on the front box, but the rear section was slotted into the bed for easy sliding. Once focused, a set screw was tightened to hold the rear section in place.[35] Having the bellows in the middle of the body facilitated making a second, in-camera copy of the original image.[36]

Daguerreotype cameras formed images on silvered copper plates. The earliest daguerreotype cameras required several minutes to half an hour to expose images on the plates. By 1840, exposure times were reduced to just a few seconds owing to improvements in the chemical preparation and development processes, and to advances in lens design.[37] American daguerreotypists introduced manufactured plates in mass production, and plate sizes became internationally standardized: whole plate (6.5 x 8.5 inches), three-quarter plate (5.5 x 7 1/8 inches), half plate (4.5 x 5.5 inches), quarter plate (3.25 x 4.25 inches), sixth plate (2.75 x 3.25 inches), and ninth plate (2 x 2.5 inches).[38] Plates were often cut to fit cases and jewelry with circular and oval shapes. Larger plates were produced, with sizes such as 9 x 13 inches (“double-whole” plate), or 13.5 x 16.5 inches (Southworth & Hawes’ plate).[39]

The collodion wet plate process that gradually replaced the daguerreotype during the 1850s required photographers to coat and sensitize thin glass or iron plates shortly before use and expose them in the camera while still wet. Early wet plate cameras were very simple and little different from Daguerreotype cameras, but more sophisticated designs eventually appeared. The Dubroni of 1864 allowed the sensitizing and developing of the plates to be carried out inside the camera itself rather than in a separate darkroom. Other cameras were fitted with multiple lenses for photographing several small portraits on a single larger plate, useful when making cartes de visite. It was during the wet plate era that the use of bellows for focusing became widespread, making the bulkier and less easily adjusted nested box design obsolete.

For many years, exposure times were long enough that the photographer simply removed the lens cap, counted off the number of seconds (or minutes) estimated to be required by the lighting conditions, then replaced the cap. As more sensitive photographic materials became available, cameras began to incorporate mechanical shutter mechanisms that allowed very short and accurately timed exposures to be made.

The use of photographic film was pioneered by George Eastman, who started manufacturing paper film in 1885 before switching to celluloid in 1889. His first camera, which he called the "Kodak," was first offered for sale in 1888. It was a very simple box camera with a fixed-focus lens and single shutter speed, which along with its relatively low price appealed to the average consumer. The Kodak came pre-loaded with enough film for 100 exposures and needed to be sent back to the factory for processing and reloading when the roll was finished. By the end of the 19th century Eastman had expanded his lineup to several models including both box and folding cameras.

Films also made possible capture of motion (cinematography) establishing the movie industry by end of 19th century.

Further information: Instant return mirror
In photography, the single-lens reflex camera (SLR) is provided with a mirror to redirect light from the picture taking lens to the viewfinder prior to releasing the shutter for composing and focusing an image. When the shutter is released, the mirror swings up and away allowing the exposure of the photographic medium and instantly returns after the exposure. No SLR camera before 1954 had this feature, although the mirror on some early SLR cameras was entirely operated by the force exerted on the shutter release and only returned when the finger pressure was released.[40][41] The Asahiflex II, released by Japanese company Asahi (Pentax) in 1954, was the world's first SLR camera with an instant return mirror.

e landline telephone contains a switchhook (A4) and an alerting device, usually a ringer (A7), that remains connected to the phone line whenever the phone is "on hook" (i.e. the switch (A4) is open), and other components which are connected when the phone is "off hook". The off-hook components include a transmitter (microphone, A2), a receiver (speaker, A1), and other circuits for dialing, filtering (A3), and amplification.

A calling party wishing to speak to another party will pick up the telephone's handset, thereby operating a lever which closes the switchhook (A4), which powers the telephone by connecting the transmitter (microphone), receiver (speaker), and related audio components to the line. The off-hook circuitry has a low resistance (less than 300 ohms) which causes a direct current (DC), which comes down the line (C) from the telephone exchange. The exchange detects this current, attaches a digit receiver circuit to the line, and sends a dial tone to indicate readiness. On a modern push-button telephone, the caller then presses the number keys to send the telephone number of the called party. The keys control a tone generator circuit (not shown) that makes DTMF tones that the exchange receives. A rotary-dial telephone uses pulse dialing, sending electrical pulses, that the exchange can count to get the telephone number (as of 2010 many exchanges were still equipped to handle pulse dialing). If the called party's line is available, the exchange sends an intermittent ringing signal (about 75 volts alternating current (AC) in North America and UK and 60 volts in Germany) to alert the called party to an incoming call. If the called party's line is in use, the exchange returns a busy signal to the calling party. However, if the called party's line is in use but has call waiting installed, the exchange sends an intermittent audible tone to the called party to indicate an incoming call.

The ringer of a telephone (A7) is connected to the line through a capacitor (A6), which blocks direct current but passes the alternating current of the ringing signal. The telephone draws no current when it is on hook, while a DC voltage is continually applied to the line. Exchange circuitry (D2) can send an AC current down the line to activate the ringer and announce an incoming call. When there is no automatic exchange, telephones have hand-cranked magnetos to generate a ringing voltage back to the exchange or any other telephone on the same line. When a landline telephone is inactive (on hook), the circuitry at the telephone exchange detects the absence of direct current to indicate that the line is not in use. When a party initiates a call to this line, the exchange sends the ringing signal. When the called party picks up the handset, they actuate a double-circuit switchhook (not shown) which may simultaneously disconnects the alerting device and connects the audio circuitry to the line. This, in turn, draws direct current through the line, confirming that the called phone is now active. The exchange circuitry turns off the ring signal, and both telephones are now active and connected through the exchange. The parties may now converse as long as both phones remain off hook. When a party hangs up, placing the handset back on the cradle or hook, direct current ceases in that line, signaling the exchange to disconnect the call.

Calls to parties beyond the local exchange are carried over trunk lines which establish connections between exchanges. In modern telephone networks, fiber-optic cable and digital technology are often employed in such connections. Satellite technology may be used for communication over very long distances.

In most landline telephones, the transmitter and receiver (microphone and speaker) are located in the handset, although in a speakerphone these components may be located in the base or in a separate enclosure. Powered by the line, the microphone (A2) produces a modulated electric current which varies its frequency and amplitude in response to the sound waves arriving at its diaphragm. The resulting current is transmitted along the telephone line to the local exchange then on to the other phone (via the local exchange or via a larger network), where it passes through the coil of the receiver (A3). The varying current in the coil produces a corresponding movement of the receiver's diaphragm, reproducing the original sound waves present at the transmitter.

Along with the microphone and speaker, additional circuitry is incorporated to prevent the incoming speaker signal and the outgoing microphone signal from interfering with each other. This is accomplished through a hybrid coil (A3). The incoming audio signal passes through a resistor (A8) and the primary winding of the coil (A3) which passes it to the speaker (A1). Since the current path A8 – A3 has a far lower impedance than the microphone (A2), virtually all of the incoming signal passes through it and bypasses the microphone.

At the same time the DC voltage across the line causes a DC current which is split between the resistor-coil (A8-A3) branch and the microphone-coil (A2-A3) branch. The DC current through the resistor-coil branch has no effect on the incoming audio signal. But the DC current passing through the microphone is turned into AC current (in response to voice sounds) which then passes through only the upper branch of the coil's (A3) primary winding, which has far fewer turns than the lower primary winding. This causes a small portion of the microphone output to be fed back to the speaker, while the rest of the AC current goes out through the phone line.

A lineman's handset is a telephone designed for testing the telephone network, and may be attached directly to aerial lines and other infrastructure components.

Early telephones were technically diverse. Some used a liquid transmitter, some had a metal diaphragm that induced current in an electromagnet wound around a permanent magnet, and some were "dynamic" - their diaphragm vibrated a coil of wire in the field of a permanent magnet or the coil vibrated the diaphragm. The sound-powered dynamic kind survived in small numbers through the 20th century in military and maritime applications, where its ability to create its own electrical power was crucial. Most, however, used the Edison/Berliner carbon transmitter, which was much louder than the other kinds, even though it required an induction coil which was an impedance matching transformer to make it compatible with the impedance of the line. The Edison patents kept the Bell monopoly viable into the 20th century, by which time the network was more important than the instrument.

Early telephones were locally powered, using either a dynamic transmitter or by the powering of a transmitter with a local battery. One of the jobs of outside plant personnel was to visit each telephone periodically to inspect the battery. During the 20th century, "common battery" operation came to dominate, powered by "talk battery" from the telephone exchange over the same wires that carried the voice signals.

Early telephones used a single wire for the subscriber's line, with ground return used to complete the circuit (as used in telegraphs). The earliest dynamic telephones also had only one port opening for sound, with the user alternately listening and speaking (or rather, shouting) into the same hole. Sometimes the instruments were operated in pairs at each end, making conversation more convenient but also more expensive.

At first, the benefits of a telephone exchange were not exploited. Instead telephones were leased in pairs to a subscriber, who had to arrange for a telegraph contractor to construct a line between them, for example between a home and a shop. Users who wanted the ability to speak to several different locations would need to obtain and set up three or four pairs of telephones. Western Union, already using telegraph exchanges, quickly extended the principle to its telephones in New York City and San Francisco, and Bell was not slow in appreciating the potential.

Signalling began in an appropriately primitive manner. The user alerted the other end, or the exchange operator, by whistling into the transmitter. Exchange operation soon resulted in telephones being equipped with a bell in a ringer box, first operated over a second wire, and later over the same wire, but with a condenser (capacitor) in series with the bell coil to allow the AC ringer signal through while still blocking DC (keeping the phone "on hook"). Telephones connected to the earliest Strowger automatic exchanges had seven wires, one for the knife switch, one for each telegraph key, one for the bell, one for the push-button and two for speaking. Large wall telephones in the early 20th century usually incorporated the bell, and separate bell boxes for desk phones dwindled away in the middle of the century.

Rural and other telephones that were not on a common battery exchange had a magneto hand-cranked generator to produce a high voltage alternating signal to ring the bells of other telephones on the line and to alert the operator. Some local farming communities that were not connected to the main networks set up barbed wire telephone lines that exploited the existing system of field fences to transmit the signal.

In the 1890s a new smaller style of telephone was introduced, packaged in three parts. The transmitter stood on a stand, known as a "candlestick" for its shape. When not in use, the receiver hung on a hook with a switch in it, known as a "switchhook". Previous telephones required the user to operate a separate switch to connect either the voice or the bell. With the new kind, the user was less likely to leave the phone "off the hook". In phones connected to magneto exchanges, the bell, induction coil, battery and magneto were in a separate bell box or "ringer box".[7] In phones connected to common battery exchanges, the ringer box was installed under a desk, or other out of the way place, since it did not need a battery or magneto.

Cradle designs were also used at this time, having a handle with the receiver and transmitter attached, now called a handset, separate from the cradle base that housed the magneto crank and other parts. They were larger than the "candlestick" and more popular.

Disadvantages of single wire operation such as crosstalk and hum from nearby AC power wires had already led to the use of twisted pairs and, for long distance telephones, four-wire circuits. Users at the beginning of the 20th century did not place long distance calls from their own telephones but made an appointment to use a special soundproofed long distance telephone booth furnished with the latest technology.

What turned out to be the most popular and longest lasting physical style of telephone was introduced in the early 20th century, including Bell's 202-type desk set. A carbon granule transmitter and electromagnetic receiver were united in a single molded plastic handle, which when not in use sat in a cradle in the base unit. The circuit diagram of the model 202 shows the direct connection of the transmitter to the line, while the receiver was induction coupled. In local battery configurations, when the local loop was too long to provide sufficient current from the exchange, the transmitter was powered by a local battery and inductively coupled, while the receiver was included in the local loop.[8] The coupling transformer and the ringer were mounted in a separate enclosure, called the subscriber set. The dial switch in the base interrupted the line current by repeatedly but very briefly disconnecting the line 1 to 10 times for each digit, and the hook switch (in the center of the circuit diagram) disconnected the line and the transmitter battery while the handset was on the cradle.

In the 1930s, telephone sets were developed that combined the bell and induction coil with the desk set, obviating a separate ringer box. The rotary dial becoming commonplace in the 1930s in many areas enabled customer-dialed service, but some magneto systems remained even into the 1960s. After World-War II, the telephone networks saw rapid expansion and more efficient telephone sets, such as the model 500 telephone in the United States, were developed that permitted larger local networks centered around central offices. A breakthrough new technology was the introduction of Touch-Tone signaling using push-button telephones by American Telephone & Telegraph Company (AT&T) in 1963.

The chair has been used since antiquity, although for many centuries it was a symbolic article of state and dignity rather than an article for ordinary use. "The chair" is still used as the emblem of authority in the House of Commons in the United Kingdom[16] and Canada,[17] and in many other settings. In keeping with this historical connotation of the "chair" as the symbol of authority, committees, boards of directors, and academic departments all have a 'chairman' or 'chair'.[18] Endowed professorships are referred to as chairs.[19]

It was not until the 16th century that chairs became common. Until then, people sat on chests, benches, and stools, which were the ordinary seats of everyday life. The number of chairs which have survived from an earlier date is exceedingly limited; most examples are of ecclesiastical or seigneurial origin.

Chairs were in existence since at least the Early Dynastic Period of Egypt. They were covered with cloth or leather, were made of carved wood, and were much lower than today’s chairs - chair seats were sometimes only 25 cm high.[20] In ancient Egypt chairs appear to have been of great richness and splendor. Fashioned of ebony and ivory, or of carved and gilded wood, they were covered with costly materials, magnificent patterns and supported upon representations of the legs of beasts or the figures of captives. Generally speaking, the higher ranked an individual was, the taller and more sumptuous was the chair he sat on and the greater the honor. On state occasions the pharaoh sat on a throne, often with a little footstool in front of it.[21]

The average Egyptian family seldom had chairs, and if they did, it was usually only the master of the household who sat on a chair. Among the better off, the chairs might be painted to look like the ornate inlaid and carved chairs of the rich, but the craftsmanship was usually poor.[20]

The earliest images of chairs in China are from sixth-century Buddhist murals and stele, but the practice of sitting in chairs at that time was rare. It was not until the twelfth century that chairs became widespread in China. Scholars disagree on the reasons for the adoption of the chair. The most common theories are that the chair was an outgrowth of indigenous Chinese furniture, that it evolved from a camp stool imported from Central Asia, that it was introduced to China by Christian missionaries in the seventh century, and that the chair came to China from India as a form of Buddhist monastic furniture. In modern China, unlike Korea or Japan, it is no longer common to sit at floor level.[22]

In Europe, it was owing in great measure to the Renaissance that the chair ceased to be a privilege of state and became a standard item of furniture for anyone who could afford to buy it. Once the idea of privilege faded the chair speedily came into general use. We find almost at once that the chair began to change every few years to reflect the fashions of the day.[23]

In the 1880s, chairs became more common in American households and usually there was a chair provided for every family member to sit down to dinner. By the 1830s, factory-manufactured “fancy chairs” like those by Sears. Roebuck, and Co. allowed families to purchase machined sets. With the Industrial Revolution, chairs became much more available.[24]

The 20th century saw an increasing use of technology in chair construction with such things as all-metal folding chairs, metal-legged chairs, the Slumber Chair,[citation needed] moulded plastic chairs and ergonomic chairs.[25] The recliner became a popular form, at least in part due to radio and television.

The modern movement of the 1960s produced new forms of chairs: the butterfly chair (originally called the Hardoy chair), bean bags, and the egg-shaped pod chair that turns. It also introduced the first mass-produced plastic chairs such as the Bofinger chair in 1966.[26] Technological advances led to molded plywood and wood laminate chairs, as well as chairs made of leather or polymers. Mechanical technology incorporated into the chair enabled adjustable chairs, especially for office use. Motors embedded in the chair resulted in massage chairs.

Chair design considers intended usage, ergonomics (how comfortable it is for the occupant),[31] as well as non-ergonomic functional requirements such as size, stacking ability, folding ability, weight, durability, stain resistance, and artistic design. Intended usage determines the desired seating position. "Task chairs," or any chair intended for people to work at a desk or table, including dining chairs, can only recline very slightly; otherwise the occupant is too far away from the desk or table. Dental chairs are necessarily reclined. Easy chairs for watching television or movies are somewhere in between depending on the height of the screen.

Ergonomic design distributes the weight of the occupant to various parts of the body. A seat that is higher results in dangling feet and increased pressure on the underside of the knees ("popliteal fold"). It may also result in no weight on the feet which means more weight elsewhere. A lower seat may shift too much weight to the "seat bones" ("ischial tuberosities").

A reclining seat and back will shift weight to the occupant's back. This may be more comfortable for some in reducing weight on the seat area, but may be problematic for others who have bad backs. In general, if the occupant is supposed to sit for a long time, weight needs to be taken off the seat area and thus "easy" chairs intended for long periods of sitting are generally at least slightly reclined. However, reclining may not be suitable for chairs intended for work or eating at table.

The back of the chair will support some of the weight of the occupant, reducing the weight on other parts of the body. In general, backrests come in three heights: Lower back backrests support only the lumbar region. Shoulder height backrests support the entire back and shoulders. Headrests support the head as well and are important in vehicles for preventing "whiplash" neck injuries in rear-end collisions where the head is jerked back suddenly. Reclining chairs typically have at least shoulder height backrests to shift weight to the shoulders instead of just the lower back.

Some chairs have foot rests. A stool or other simple chair may have a simple straight or curved bar near the bottom for the sitter to place his or her feet on.

Some chairs have two curved bands of wood (also known as rockers) attached to the bottom of the legs. They are called rocking chairs.


The type of chair popular in western Hubei, China: with a fairly low seat and the back inclined at about 45 degrees from the vertical
A kneeling chair adds an additional body part, the knees, to support the weight of the body. A sit-stand chair distributes most of the weight of the occupant to the feet. Many chairs are padded or have cushions. Padding can be on the seat of the chair only, on the seat and back, or also on any arm rests or foot rest the chair may have. Padding will not shift the weight to different parts of the body (unless the chair is so soft that the shape is altered). However, padding does distribute the weight by increasing the area of contact between the chair and the body. A hard wood chair feels hard because the contact point between the occupant and the chair is small. The same body weight over a smaller area means greater pressure on that area. Spreading the area reduces the pressure at any given point. In lieu of padding, flexible materials, such as wicker, may be used instead with similar effects of distributing the weight. Since most of the body weight is supported in the back of the seat, padding there should be firmer than the front of the seat which only has the weight of the legs to support. Chairs that have padding that is the same density front and back will feel soft in the back area and hard to the underside of the knees.

There may be cases where padding is not desirable. For example, in chairs that are intended primarily for outdoor use. Where padding is not desirable, contouring may be used instead. A contoured seat pan attempts to distribute weight without padding. By matching the shape of the occupant's buttocks, weight is distributed and maximum pressure is reduced.


Churchchairs
Actual chair dimensions are determined by measurements of the human body or anthropometric measurements. The two most relevant anthropometric measurement for chair design is the popliteal height and buttock popliteal length.

For someone seated, the popliteal height is the distance from the underside of the foot to the underside of the thigh at the knees. It is sometimes called the "stool height." The term "sitting height" is reserved for the height to the top of the head when seated. For American men, the median popliteal height is 16.3 inches (410 mm) and for American women it is 15.0 inches (380 mm).[32] The popliteal height, after adjusting for heels, clothing and other issues is used to determine the height of the chair seat. Mass-produced chairs are typically 17 inches (430 mm) high.

For someone seated, the buttock popliteal length is the horizontal distance from the back most part of the buttocks to the back of the lower leg. This anthropometric measurement is used to determine the seat depth. Mass-produced chairs are typically 15-17 inches deep.

Additional anthropometric measurements may be relevant to designing a chair. Hip breadth is used for chair width and armrest width. Elbow rest height is used to determine the height of the armrests. The buttock-knee length is used to determine "leg room" between rows of chairs. "Seat pitch" is the distance between rows of seats. In some airplanes and stadiums the leg room (the seat pitch less the thickness of the seat at thigh level) is so small that it is sometimes insufficient for the average person.

For adjustable chairs, such as an office chair, the aforementioned principles are applied in adjusting the chair to the individual occupant. Caster wheels are attached to the feet of chairs to give more mobility. Gas springs are attached to the body of the chair in order to give height adjustment and more comfort to the user.

A blender consists of a housing, motor, blades, and food container. A fan-cooled electric motor is secured into the housing by way of vibration dampers, and a small output shaft penetrates the upper housing and meshes with the blade assembly. Usually, a small rubber washer provides a seal around the output shaft to prevent liquid from entering the motor. Most blenders today have multiple speeds. As a typical blender has no gearbox, the multiple speeds are often implemented using a universal motor with multiple stator windings and/or multi-tapped stator windings; in a blender with electromechanical controls, the button (or other electrical switching device or position) for each different speed connects a different stator winding/tap or combination thereof. Each different combination of energized windings produces a different torque from the motor, which yields a different equilibrium speed in balance against the drag (resistance to rotation) of the blade assembly in contact with the material inside the food container. A notable exception from the mid-1960s is the Oster Model 412 Classic VIII (with the single knob) providing the lowest speed (Stir) using the aforementioned winding tap method but furnishing higher speeds (the continuously variable higher speed range is marked Puree to Liquify) by means of a mechanical speed governor that balances the force provided by flyweights against a spring force varied by the control knob when it is switched into the higher speed range. With this arrangement, when not set to the Stir speed, motor speed is constant even with varying load up to the point where power demanded by the load is equal to the motor's power capability at a particular speed. The more modern version of this arrangement is electronic speed control found on some units.

